{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use Convolutional neural networks or CNN to classify data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_image, train_labels) , (test_image, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "   keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "   keras.layers.MaxPooling2D(2, 2),\n",
    "   keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "   keras.layers.MaxPooling2D(2,2),\n",
    "   keras.layers.Flatten(),\n",
    "   keras.layers.Dense(128, activation='relu'),\n",
    "   keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=train_image.reshape(60000, 28, 28, 1)\n",
    "train_image=train_image / 255.0\n",
    "test_image = test_image.reshape(10000, 28, 28, 1)\n",
    "test_image=test_image/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class myCallback(tf.keras.callbacks.Callback):\n",
    "    #def on_epoch_end(self , epochs, logs={}):\n",
    "        #if (logs.get('loss')<0.05):\n",
    "            #print('\\nLoss is low thus stopping training')\n",
    "            #self.model.stop_training=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 75s 40ms/step - loss: 0.4328 - accuracy: 0.8437\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 74s 40ms/step - loss: 0.2876 - accuracy: 0.8939\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 76s 40ms/step - loss: 0.2427 - accuracy: 0.9101\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 75s 40ms/step - loss: 0.2109 - accuracy: 0.9204\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.1846 - accuracy: 0.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d2295bed08>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#callbacks = myCallback()\n",
    "model.summary()\n",
    "model.fit(train_image, train_labels, epochs=5) #callbacks=[callbacks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RMSprop in module tensorflow.python.keras.optimizer_v2.rmsprop:\n",
      "\n",
      "class RMSprop(tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2)\n",
      " |  RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n",
      " |  \n",
      " |  Optimizer that implements the RMSprop algorithm.\n",
      " |  \n",
      " |  A detailed description of rmsprop.\n",
      " |    - maintain a moving (discounted) average of the square of gradients\n",
      " |    - divide gradient by the root of this average\n",
      " |  \n",
      " |  The default settings does not use momentum:\n",
      " |  \n",
      " |  $$rms_t = \\rho * rms_{t-1} + (1-\\rho) * g_t^2$$\n",
      " |  $$\\theta_t = \\theta_{t-1} - \\mathrm{learning\\_rate} *\n",
      " |                              g_t / \\sqrt{rms_t + \\epsilon}$$\n",
      " |  \n",
      " |  Since  $x/x^2 = sign(x)$, this  is an smoothed approximation of:\n",
      " |  \n",
      " |  $$ \\theta_t = \\theta_{t-1} - \\mathrm{learning\\_rate} * sign(g_t) $$\n",
      " |  \n",
      " |  With momentum the update is:\n",
      " |  \n",
      " |  $$rms_t = \\rho * rms_{t-1} + (1-\\rho) * g_t^2$$\n",
      " |  $$mom_t = \\mathrm{momentum} * mom_{t-1} + g_t / \\sqrt{rms_t + \\epsilon}$$\n",
      " |  $$\\theta_t = \\theta_{t-1} - \\mathrm{learning\\_rate} * mom_t$$\n",
      " |  \n",
      " |  This implementation of RMSprop uses plain momentum, not Nesterov momentum.\n",
      " |  \n",
      " |  The centered version additionally maintains a moving average of the\n",
      " |  gradients, and uses that average to estimate the variance:\n",
      " |  \n",
      " |  $$mg_t = \\rho * mg_{t-1} + (1-\\rho) * g_t$$\n",
      " |  $$rms_t = \\rho * rms_{t-1} + (1-\\rho) * g_t^2$$\n",
      " |  $$mom_t = \\mathrm{momentum} * mom_{t-1} +\n",
      " |      \\mathrm{learning\\_rate} * g_t / sqrt(rms_t - mg_t^2 + \\epsilon)$$\n",
      " |  $$\\theta_t = \\theta_{t-1} - mom_t$$\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  >>> opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
      " |  >>> var1 = tf.Variable(10.0)\n",
      " |  >>> loss = lambda: (var1 ** 2)/2.0                # d(loss)/d(var1) = var1\n",
      " |  >>> step_count = opt.minimize(loss, [var1]).numpy()\n",
      " |  >>> var1.numpy()\n",
      " |  9.683772\n",
      " |  \n",
      " |  References\n",
      " |    See ([pdf]\n",
      " |      http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RMSprop\n",
      " |      tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n",
      " |      Construct a new RMSprop optimizer.\n",
      " |      \n",
      " |      Note that in the dense implementation of this algorithm, variables and their\n",
      " |      corresponding accumulators (momentum, gradient moving average, square\n",
      " |      gradient moving average) will be updated even if the gradient is zero\n",
      " |      (i.e. accumulators will decay, momentum will be applied). The sparse\n",
      " |      implementation (used when the gradient is an `IndexedSlices` object,\n",
      " |      typically because of `tf.gather` or an embedding lookup in the forward pass)\n",
      " |      will not update variable slices or their accumulators unless those slices\n",
      " |      were used in the forward pass (nor is there an \"eventual\" correction to\n",
      " |      account for these omitted updates). This leads to more efficient updates for\n",
      " |      large embedding lookup tables (where most of the slices are not accessed in\n",
      " |      a particular graph execution), but differs from the published algorithm.\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
      " |          `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
      " |          that takes no arguments and returns the actual value to use. The\n",
      " |          learning rate. Defeaults to 0.001.\n",
      " |        rho: Discounting factor for the history/coming gradient. Defaults to 0.9.\n",
      " |        momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n",
      " |        epsilon: A small constant for numerical stability. This epsilon is\n",
      " |          \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |          Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
      " |          1e-7.\n",
      " |        centered: Boolean. If `True`, gradients are normalized by the estimated\n",
      " |          variance of the gradient; if False, by the uncentered second moment.\n",
      " |          Setting this to `True` may help with training, but is slightly more\n",
      " |          expensive in terms of computation and memory. Defaults to `False`.\n",
      " |        name: Optional name prefix for the operations created when applying\n",
      " |          gradients. Defaults to \"RMSprop\".  @compatibility(eager) When eager\n",
      " |          execution is enabled, `learning_rate`, `decay`, `momentum`, and\n",
      " |          `epsilon` can each be a callable that takes no arguments and returns the\n",
      " |          actual value to use. This can be useful for changing these values across\n",
      " |          different invocations of optimizer functions. @end_compatibility\n",
      " |        **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
      " |          `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
      " |          gradients by value, `decay` is included for backward compatibility to\n",
      " |          allow time inverse decay of learning rate. `lr` is included for backward\n",
      " |          compatibility, recommended to use `learning_rate` instead.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimizer.\n",
      " |      \n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Set the weights of the optimizer.\n",
      " |      \n",
      " |      The weights of an optimizer are its state (ie, variables).\n",
      " |      This function takes the weight values associated with this\n",
      " |      optimizer as a list of Numpy arrays. The first value is always the\n",
      " |      iterations count of the optimizer, followed by the optimizer's state\n",
      " |      variables in the order they are created. The passed values are used to set\n",
      " |      the new state of the optimizer.\n",
      " |      \n",
      " |      For example, the RMSprop optimizer for this simple model takes a list of\n",
      " |      three values-- the iteration count, followed by the root-mean-square value\n",
      " |      of the kernel and bias of the single Dense layer:\n",
      " |      \n",
      " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
      " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |      >>> m.compile(opt, loss='mse')\n",
      " |      >>> data = np.arange(100).reshape(5, 20)\n",
      " |      >>> labels = np.zeros(5)\n",
      " |      >>> print('Training'); results = m.fit(data, labels)\n",
      " |      Training ...\n",
      " |      >>> new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]\n",
      " |      >>> opt.set_weights(new_weights)\n",
      " |      >>> opt.iterations\n",
      " |      <tf.Variable 'RMSprop/iter:0' shape=() dtype=int64, numpy=10>\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: weight values as a list of numpy arrays.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  __getattribute__(self, name)\n",
      " |      Overridden to support hyperparameter access.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Override setattr to support dynamic hyperparameter setting.\n",
      " |  \n",
      " |  add_slot(self, var, slot_name, initializer='zeros')\n",
      " |      Add a new slot variable for `var`.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer='zeros', trainable=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      The method sums gradients from all replicas in the presence of\n",
      " |      `tf.distribute.Strategy` by default. You can aggregate gradients yourself by\n",
      " |      passing `experimental_aggregate_gradients=False`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      grads = tape.gradient(loss, vars)\n",
      " |      grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
      " |      # Processing aggregated gradients.\n",
      " |      optimizer.apply_gradients(zip(grads, vars),\n",
      " |          experimental_aggregate_gradients=False)\n",
      " |      \n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs.\n",
      " |        name: Optional name for the returned operation. Default to the name passed\n",
      " |          to the `Optimizer` constructor.\n",
      " |        experimental_aggregate_gradients: Whether to sum gradients from different\n",
      " |          replicas in the presense of `tf.distribute.Strategy`. If False, it's\n",
      " |          user responsibility to aggregate the gradients. Default to True.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. The `iterations`\n",
      " |        will be automatically increased by 1.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |  \n",
      " |  get_gradients(self, loss, params)\n",
      " |      Returns gradients of `loss` with respect to `params`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        loss: Loss tensor.\n",
      " |        params: List of variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of gradient tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
      " |          function not implemented).\n",
      " |  \n",
      " |  get_slot(self, var, slot_name)\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      A list of names for this optimizer's slots.\n",
      " |  \n",
      " |  get_updates(self, loss, params)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the optimizer.\n",
      " |      \n",
      " |      The weights of an optimizer are its state (ie, variables).\n",
      " |      This function returns the weight values associated with this\n",
      " |      optimizer as a list of Numpy arrays. The first value is always the\n",
      " |      iterations count of the optimizer, followed by the optimizer's state\n",
      " |      variables in the order they were created. The returned list can in turn\n",
      " |      be used to load state into similarly parameterized optimizers.\n",
      " |      \n",
      " |      For example, the RMSprop optimizer for this simple model returns a list of\n",
      " |      three values-- the iteration count, followed by the root-mean-square value\n",
      " |      of the kernel and bias of the single Dense layer:\n",
      " |      \n",
      " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
      " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |      >>> m.compile(opt, loss='mse')\n",
      " |      >>> data = np.arange(100).reshape(5, 20)\n",
      " |      >>> labels = np.zeros(5)\n",
      " |      >>> print('Training'); results = m.fit(data, labels)\n",
      " |      Training ...\n",
      " |      >>> len(opt.get_weights())\n",
      " |      3\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  minimize(self, loss, var_list, grad_loss=None, name=None)\n",
      " |      Minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A callable taking no arguments which returns the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable` objects.\n",
      " |          Use callable when the variable list would otherwise be incomplete before\n",
      " |          `minimize` since the variables are created at the first time `loss` is\n",
      " |          called.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |        name: Optional name for the returned operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that updates the variables in `var_list`. The `iterations`\n",
      " |        will be automatically increased by 1.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  variables(self)\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from abc.ABCMeta\n",
      " |      Creates an optimizer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same optimizer from the config\n",
      " |      dictionary.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional Python\n",
      " |            objects used to create this optimizer, such as a function used for a\n",
      " |            hyperparameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  iterations\n",
      " |      Variable. The number of training steps this Optimizer has run.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
